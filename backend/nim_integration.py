"""
NIM Integration - Offline Feature Extraction + Online Cache (Fixed v3)
- NIM offline extraction on GPU 1 (every 6 hours)
- Online cache retrieval from PGVector (< 10ms)
- SQL Injection FIX: Using make_interval for parameterized queries
"""
import asyncio
import psycopg2
import numpy as np
import logging
import os
import requests
import hashlib

try:
    from pgvector.psycopg2 import register_vector  # type: ignore
except Exception:  # pragma: no cover - optional dependency in non-pgvector environments
    register_vector = None

logger = logging.getLogger(__name__)

class NIMFeatureCache:
    """
    NIM Offline Feature Cache (Fixed v3):
    - Offline: Extract semantic features every 6 hours (GPU 1)
    - Online: Read cached features from PGVector (GPU 0)
    - FIXED: SQL injection vulnerability with make_interval
    - Added: last_news_time tracking
    """

    def __init__(self, db_url: str):
        self.conn = psycopg2.connect(db_url)
        self.vector_enabled = False
        self.vector_adapter_ready = False
        self.vector_column_type = "double precision[]"
        with self.conn.cursor() as cur:
            try:
                cur.execute("CREATE EXTENSION IF NOT EXISTS vector")
                self.conn.commit()
                self.vector_enabled = True
                self.vector_column_type = "vector(384)"
            except Exception as exc:
                self.conn.rollback()
                logger.warning("pgvector extension unavailable, fallback to float8[]: %s", exc)
        if self.vector_enabled and register_vector is not None:
            try:
                register_vector(self.conn)
                self.vector_adapter_ready = True
            except Exception as exc:
                logger.warning("pgvector adapter register failed, use text cast fallback: %s", exc)
        self._ensure_table()

    def _ensure_table(self):
        """Ensure PGVector table exists (FIXED: added last_news_time)"""
        with self.conn.cursor() as cur:
            cur.execute(f"""
                CREATE TABLE IF NOT EXISTS semantic_features (
                    symbol VARCHAR(20) PRIMARY KEY,
                    feature_vector {self.vector_column_type},
                    timestamp TIMESTAMP DEFAULT NOW(),
                    last_news_time TIMESTAMP,  -- âœ… NEW: Last news timestamp
                    news_count INTEGER
                )
            """)
            self.conn.commit()

    def _adapt_feature_vector(self, feature_vector: list):
        vals = [float(x) for x in feature_vector]
        if not self.vector_enabled:
            return vals
        if self.vector_adapter_ready:
            return vals
        return "[" + ",".join(f"{x:.8f}" for x in vals) + "]"

    async def offline_extraction(self, symbol: str, news_items: list):
        """
        Offline feature extraction (every 6 hours, GPU 1)
        """
        if not news_items:
            logger.warning(f"âš ï¸  {symbol}: No news, skipping extraction")
            return

        # Filter low-quality news
        filtered_news = self._filter_low_quality(news_items)
        if not filtered_news:
            logger.warning(f"âš ï¸  {symbol}: No valid news after filtering")
            return

        # Build batch prompt
        prompt = self._build_batch_prompt(filtered_news)

        # Call NIM to generate feature vector (single API call)
        feature_vector = await self._call_nim_embedding(prompt)

        # Store in PGVector (FIXED: added last_news_time)
        with self.conn.cursor() as cur:
            try:
                cur.execute("""
                    INSERT INTO semantic_features
                    (symbol, feature_vector, timestamp, last_news_time, news_count)
                    VALUES (%s, %s, NOW(), %s, %s)
                    ON CONFLICT (symbol) DO UPDATE SET
                        feature_vector = EXCLUDED.feature_vector,
                        timestamp = EXCLUDED.timestamp,
                        last_news_time = EXCLUDED.last_news_time,
                        news_count = EXCLUDED.news_count
                """, (
                    symbol,
                    self._adapt_feature_vector(feature_vector),
                    filtered_news[0].get('time'),
                    len(filtered_news)
                ))
                self.conn.commit()
                logger.info(f"âœ… {symbol}: Extracted semantic features from {len(filtered_news)} news items")
            except Exception as e:
                logger.error(f"âŒ {symbol}: Failed to store features: {e}")
                self.conn.rollback()

    def get_cached_features(self, symbol: str, max_age_hours: int = 6):
        """
        Online retrieval of cached features (real-time prediction, GPU 0)

        Returns: feature_vector or None (if expired)
        """
        with self.conn.cursor() as cur:
            try:
                # âœ… FIXED SQL Injection: Using make_interval with parameterization
                cur.execute("""
                    SELECT feature_vector, last_news_time, news_count
                    FROM semantic_features
                    WHERE symbol = %s
                    AND timestamp > NOW() - make_interval(secs => %s * 3600)
                """, (symbol, max_age_hours))

                result = cur.fetchone()
                if result:
                    feature_vector, last_news_time, news_count = result
                    logger.debug(f"ðŸ“¦ {symbol}: Retrieved cached features (age < {max_age_hours}h)")
                    return feature_vector
                logger.info(f"â° {symbol}: No cached features (expired or not found)")
                return None
            except Exception as e:
                logger.error(f"âŒ {symbol}: Failed to retrieve cached features: {e}")
                return None

    def _filter_low_quality(self, news_items: list) -> list:
        """Filter low-quality news"""
        filtered = []
        for item in news_items:
            title = item.get('title', '')
            if len(title) < 10:
                continue
            if 'å¹¿å‘Š' in title or 'æŽ¨å¹¿' in title:
                continue
            filtered.append(item)
        return filtered

    def _build_batch_prompt(self, news_items: list) -> str:
        """Build batch news analysis prompt"""
        texts = [f"{i+1}. {item['title']} - {item.get('summary', '')}"
                for i, item in enumerate(news_items[:20])]

        return f"""Analyze the comprehensive market impact of the following {len(texts)} financial news items:

{chr(10).join(texts)}

Extract key investment insights and output structured analysis results."""

    async def _call_nim_embedding(self, prompt: str) -> list:
        """
        Call NIM to generate embedding vector
        Cost estimate: ~Â¥0.01 per call
        """
        endpoint = os.getenv("NIM_EMBED_ENDPOINT")
        api_key = os.getenv("NIM_API_KEY")
        model = os.getenv("NIM_EMBED_MODEL", "nv-embed-v1")
        if endpoint:
            try:
                headers = {"Content-Type": "application/json"}
                if api_key:
                    headers["Authorization"] = f"Bearer {api_key}"
                payload = {"input": prompt, "model": model}
                resp = await asyncio.to_thread(
                    requests.post,
                    endpoint,
                    headers=headers,
                    json=payload,
                    timeout=30,
                )
                resp.raise_for_status()
                data = resp.json()
                if isinstance(data, dict):
                    emb = data.get("embedding")
                    if emb is None and isinstance(data.get("data"), list) and data["data"]:
                        emb = data["data"][0].get("embedding")
                    if isinstance(emb, list) and emb:
                        if len(emb) >= 384:
                            return [float(x) for x in emb[:384]]
                        return [float(x) for x in emb] + [0.0] * (384 - len(emb))
            except Exception as e:
                logger.warning(f"NIM endpoint failed, fallback to deterministic local embedding: {e}")

        # deterministic fallback instead of random for reproducibility
        await asyncio.sleep(0.005)
        seed = int(hashlib.sha256(prompt.encode("utf-8")).hexdigest()[:8], 16)
        rs = np.random.default_rng(seed)
        return rs.standard_normal(384).astype(np.float32).tolist()

    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()
            logger.info("âœ… NIM feature cache connection closed")

# Global instance
_nim_cache = None

def get_nim_cache(db_url: str = None):
    """Get global NIM cache instance"""
    global _nim_cache
    if _nim_cache is None and db_url:
        _nim_cache = NIMFeatureCache(db_url)
    return _nim_cache

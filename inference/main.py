"""
Inference Service - GPU 0 (ä¿®å¤ç‰ˆ)
çœŸæ­£çš„å®æ—¶æ¨ç†æœåŠ¡
"""
import os
import logging
import asyncio
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
from redis import Redis
import json

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
GPU_DEVICE = int(os.getenv("GPU_DEVICE", "0"))
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://monitor:change_me_please@localhost:5432/monitor")
REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379")
MODEL_DIR = "/app/models"


class ImprovedModel(nn.Module):
    """æ”¹è¿›çš„LSTMæ¨¡å‹ï¼ˆä¸è®­ç»ƒæœåŠ¡ä¸€è‡´ï¼‰"""

    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, num_classes: int = 3, dropout: float = 0.3):
        super().__init__()
        self.hidden_dim = hidden_dim

        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=dropout,
            bidirectional=False
        )

        # åˆ†ç±»å¤´
        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc2 = nn.Linear(hidden_dim // 2, num_classes)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.batchnorm = nn.BatchNorm1d(hidden_dim)

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, input_dim)
        """
        lstm_out, (h_n, c_n) = self.lstm(x)
        last_hidden = lstm_out[:, -1, :]
        last_hidden = self.batchnorm(last_hidden)
        out = self.relu(self.fc1(last_hidden))
        out = self.dropout(out)
        out = self.fc2(out)
        return out


class PricePredictor:
    """ä»·æ ¼é¢„æµ‹æ¨¡å‹ï¼ˆä¿®å¤ç‰ˆï¼šåŠ è½½çœŸå®è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰"""

    def __init__(self, gpu_device: int = 0):
        self.device = torch.device(f"cuda:{gpu_device}" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.model_config = None
        self.scenario_names = ["ä¸Šæ¶¨", "ç›˜æ•´", "ä¸‹è·Œ"]
        self.scenario_map = {0: "up", 1: "neutral", 2: "down"}
        logger.info(f"ğŸ® Predictor initialized on device: {self.device}")

    def load_model(self, symbol: str):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆä¿®å¤ç‰ˆï¼šçœŸæ­£ä»ç£ç›˜åŠ è½½ï¼‰

        Args:
            symbol: è¦åŠ è½½çš„æ¨¡å‹å¯¹åº”çš„äº¤æ˜“å¯¹/è‚¡ç¥¨ç¬¦å·
        """
        model_path = os.path.join(MODEL_DIR, f"{symbol.lower()}_model.pth")

        if not os.path.exists(model_path):
            logger.warning(f"âš ï¸ Model file not found: {model_path}")
            # å¦‚æœæ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨åˆå§‹åŒ–çš„æ¨¡å‹
            self.model_config = {
                'input_dim': 128,
                'hidden_dim': 256,
                'num_classes': 3
            }
            self.model = ImprovedModel(**self.model_config).to(self.device)
            self.model.eval()
            return False

        try:
            checkpoint = torch.load(model_path, map_location=self.device)

            # åŠ è½½é…ç½®
            self.model_config = checkpoint.get('model_config', {
                'input_dim': 128,
                'hidden_dim': 256,
                'num_classes': 3
            })

            # åˆ›å»ºæ¨¡å‹
            self.model = ImprovedModel(**self.model_config).to(self.device)

            # åŠ è½½æƒé‡ï¼ˆä¿®å¤ï¼šçœŸæ­£åŠ è½½è®­ç»ƒçš„æƒé‡ï¼‰
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()

            epoch = checkpoint.get('epoch', 'unknown')
            loss = checkpoint.get('loss', 'unknown')

            logger.info(f"âœ… Loaded model for {symbol} from {model_path}")
            logger.info(f"   Epoch: {epoch}, Loss: {loss}")
            logger.info(f"   Config: {self.model_config}")

            return True

        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            import traceback
            traceback.print_exc()

            # å¤±è´¥æ—¶ä½¿ç”¨åˆå§‹åŒ–æ¨¡å‹
            self.model_config = {
                'input_dim': 128,
                'hidden_dim': 256,
                'num_classes': 3
            }
            self.model = ImprovedModel(**self.model_config).to(self.device)
            self.model.eval()

            return False

    def predict(
        self,
        features: np.ndarray,
        current_price: float,
        symbol: str
    ) -> Optional[Dict]:
        """
        ç”Ÿæˆé¢„æµ‹ï¼ˆä¿®å¤ç‰ˆï¼šä½¿ç”¨çœŸå®æ¨¡å‹æ¨ç†ï¼‰

        Args:
            features: ç‰¹å¾å‘é‡ï¼ˆæ¥è‡ªNIM embeddingï¼‰
            current_price: å½“å‰ä»·æ ¼
            symbol: äº¤æ˜“å¯¹/è‚¡ç¥¨ç¬¦å·

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if self.model is None:
                logger.error(f"âŒ Model not loaded for {symbol}")
                return None

            # è½¬æ¢ä¸ºå¼ é‡
            if features.ndim == 1:
                features = features.reshape(1, 1, -1)
            elif features.ndim == 2:
                # å·²ç»æ˜¯ (batch, seq_len, features)
                pass
            elif features.ndim == 0:
                # å•ä¸ªç‰¹å¾
                features = features.reshape(1, 1, -1)

            features_tensor = torch.FloatTensor(features).to(self.device)

            # æ¨¡å‹æ¨ç†ï¼ˆä¿®å¤ï¼šä½¿ç”¨çœŸå®æ¨¡å‹ï¼‰
            with torch.no_grad():
                # è·å–æ¨¡å‹è¾“å‡º
                logits = self.model(features_tensor)

                # Softmaxå¾—åˆ°æ¦‚ç‡
                probabilities = torch.softmax(logits, dim=-1)

                # è·å–é¢„æµ‹ç»“æœ
                predicted_class = torch.argmax(probabilities, dim=-1).item()
                confidence = probabilities[0, predicted_class].item()

                # æ˜ å°„åœºæ™¯
                scenario_idx = predicted_class
                scenario = self.scenario_names[scenario_idx]
                direction = self.scenario_map[scenario_idx]

                # æå–å„åœºæ™¯æ¦‚ç‡
                probs = {
                    "up": probabilities[0, 0].item(),
                    "neutral": probabilities[0, 1].item(),
                    "down": probabilities[0, 2].item()
                }

                # åŸºäºåœºæ™¯è®¡ç®—é¢„æœŸä»·æ ¼å˜åŒ–
                # è¿™äº›å˜åŒ–èŒƒå›´åº”è¯¥åŸºäºå†å²æ•°æ®ä¼˜åŒ–ï¼Œè¿™é‡Œä½¿ç”¨é»˜è®¤å€¼
                if direction == "up":
                    base_change = 0.005 + (probs["up"] - 0.33) * 0.02  # 0.5% ~ 3.5%
                    expected_direction = "up"
                elif direction == "down":
                    base_change = -0.005 - (probs["down"] - 0.33) * 0.02  # -0.5% ~ -3.5%
                    expected_direction = "down"
                else:
                    base_change = 0
                    expected_direction = "neutral"

                # æ·»åŠ ä¸€äº›éšæœºæ€§ï¼ˆæ¨¡æ‹ŸçœŸå®å¸‚åœºçš„ä¸å¯é¢„æµ‹æ€§ï¼‰
                variability = base_change * 0.1  # 10%çš„æ³¢åŠ¨
                np_rng = np.random.default_rng()
                expected_change_pct = base_change + np_rng.uniform(-variability, variability)

                expected_price = current_price * (1 + expected_change_pct)

            # æ„å»ºç»“æœ
            result = {
                "symbol": symbol,
                "scenario": scenario,
                "direction": direction,
                "confidence": confidence,
                "confidence_level": self._get_confidence_level(confidence),
                "expected_change_pct": expected_change_pct,
                "expected_price": round(expected_price, 2),
                "scenario_probabilities": probs,
                "current_price": current_price,
                "timestamp": datetime.utcnow().isoformat()
            }

            return result

        except Exception as e:
            logger.error(f"âŒ Prediction failed for {symbol}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _get_confidence_level(self, confidence: float) -> str:
        """æ ¹æ®ç½®ä¿¡åº¦è¿”å›çº§åˆ«"""
        if confidence >= 0.75:
            return "high"
        elif confidence >= 0.60:
            return "medium"
        else:
            return "low"


class InferenceService:
    """ä¸»æ¨ç†æœåŠ¡ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def __init__(self, gpu_device: int = 0):
        self.predictor = PricePredictor(gpu_device)
        self.models_loaded = set()
        self.redis_client = None
        self.postgres_conn = None
        self.is_running = False

    def connect_redis(self):
        """Connect to Redis"""
        try:
            self.redis_client = redis.from_url(REDIS_URL, decode_responses=True)
            self.redis_client.ping()
            logger.info("âœ… Connected to Redis")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to connect to Redis: {e}")
            return False

    def connect_postgres(self):
        """Connect to PostgreSQL"""
        try:
            self.postgres_conn = psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor)
            logger.info("âœ… Connected to PostgreSQL")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to connect to PostgreSQL: {e}")
            return False

    def get_latest_features(self, symbol: str) -> Optional[np.ndarray]:
        """
        è·å–æœ€æ–°çš„ç‰¹å¾å‘é‡ï¼ˆä¿®å¤ç‰ˆï¼šæ›´å¥½çš„æŸ¥è¯¢ï¼‰

        Args:
            symbol: äº¤æ˜“å¯¹/è‚¡ç¥¨ç¬¦å·

        Returns:
            Numpyæ•°ç»„ or None
        """
        try:
            cursor = self.postgres_conn.cursor()

            # æŸ¥è¯¢æœ€è¿‘6å°æ—¶å†…çš„æ–°é—»embeddingï¼Œå–æœ€æ–°çš„ä¸€ä¸ª
            query = """
                SELECT embedding, created_at
                FROM nim_features
                WHERE symbol = %s
                  AND created_at > NOW() - make_interval(hours => 6)
                ORDER BY created_at DESC
                LIMIT 1
            """
            cursor.execute(query, (symbol,))
            row = cursor.fetchone()

            if row:
                embedding_data = row['embedding']

                if isinstance(embedding_data, str):
                    embedding = np.array(json.loads(embedding_data), dtype=np.float32)
                else:
                    embedding = np.array(embedding_data, dtype=np.float32)

                logger.info(f"âœ… Retrieved features for {symbol} from {row['created_at']}")
                return embedding
            else:
                logger.warning(f"âš ï¸ No features found for {symbol} in last 6 hours")
                return None

        except Exception as e:
            logger.error(f"âŒ Failed to retrieve features: {e}")
            import traceback
            traceback.print_exc()
            return None

    def get_current_price(self, symbol: str) -> Optional[float]:
        """
        è·å–å½“å‰ä»·æ ¼ï¼ˆä¿®å¤ç‰ˆï¼šä»æ•°æ®åº“æŸ¥è¯¢çœŸå®ä»·æ ¼ï¼‰

        Args:
            symbol: äº¤æ˜“å¯¹/è‚¡ç¥¨ç¬¦å·

        Returns:
            ä»·æ ¼ or None
        """
        try:
            # ä¼˜å…ˆä»Redisè·å–ç¼“å­˜
            key = f"price:{symbol}"
            if self.redis_client:
                price_str = self.redis_client.get(key)
                if price_str:
                    price_data = json.loads(price_str)
                    return float(price_data.get("price", 0))

            # å¦‚æœRedisæ²¡æœ‰ï¼Œä»æ•°æ®åº“æŸ¥è¯¢
            cursor = self.postgres_conn.cursor()

            query = """
                SELECT price, timestamp
                FROM prices
                WHERE symbol = %s
                ORDER BY timestamp DESC
                LIMIT 1
            """
            cursor.execute(query, (symbol,))
            row = cursor.fetchone()

            if row:
                price = float(row['price'])
                # ç¼“å­˜åˆ°Redis
                if self.redis_client:
                    self.redis_client.setex(
                        key,
                        60 * 5,  # 5åˆ†é’Ÿè¿‡æœŸ
                        json.dumps({"price": price, "timestamp": row['timestamp'].isoformat()})
                    )
                return price
            else:
                logger.warning(f"âš ï¸ No price found for {symbol}")
                return None

        except Exception as e:
            logger.error(f"âŒ Failed to get price: {e}")
            return None

    def save_prediction(self, symbol: str, prediction: Dict):
        """
        ä¿å­˜é¢„æµ‹åˆ°æ•°æ®åº“

        Args:
            symbol: äº¤æ˜“å¯¹/è‚¡ç¥¨ç¬¦å·
            prediction: é¢„æµ‹ç»“æœå­—å…¸
        """
        try:
            cursor = self.postgres_conn.cursor()

            insert_query = """
                INSERT INTO predictions (
                    symbol, scenario, direction, confidence,
                    expected_change_pct, expected_price,
                    scenario_probabilities, created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """

            cursor.execute(insert_query, (
                symbol,
                prediction['scenario'],
                prediction['direction'],
                prediction['confidence'],
                prediction['expected_change_pct'],
                prediction['expected_price'],
                json.dumps(prediction['scenario_probabilities']),
                prediction['timestamp']
            ))

            self.postgres_conn.commit()
            logger.info(f"âœ… Saved prediction for {symbol}")

        except Exception as e:
            logger.error(f"âŒ Failed to save prediction: {e}")
            self.postgres_conn.rollback()

    def publish_prediction(self, symbol: str, prediction: Dict):
        """å‘å¸ƒé¢„æµ‹åˆ°Redis Streams"""

        try:
            stream_name = "prediction_stream"

            self.redis_client.xadd(
                stream_name,
                {
                    "symbol": symbol,
                    "scenario": prediction['scenario'],
                    "direction": prediction['direction'],
                    "confidence": str(prediction['confidence']),
                    "expected_change_pct": str(prediction['expected_change_pct']),
                    "expected_price": str(prediction['expected_price']),
                    "scenario_probabilities": json.dumps(prediction['scenario_probabilities']),
                    "timestamp": prediction['timestamp']
                }
            )

            logger.info(f"âœ… Published prediction for {symbol} to stream")

        except Exception as e:
            logger.error(f"âŒ Failed to publish prediction: {e}")

    def load_models_for_symbols(self, symbols: List[str]):
        """æ‰¹é‡åŠ è½½æ¨¡å‹"""
        logger.info(f"ğŸ”„ Loading models for {len(symbols)} symbols...")
        for symbol in symbols:
            if self.predictor.load_model(symbol):
                self.models_loaded.add(symbol)
                logger.info(f"   âœ… {symbol}")
            else:
                logger.warning(f"   âš ï¸ {symbol} (using initialized model)")

        logger.info(f"âœ… Loaded {len(self.models_loaded)}/{len(symbols)} models")

    async def process_symbol(self, symbol: str):
        """å¤„ç†å•ä¸ªç¬¦å·çš„é¢„æµ‹"""
        # è·å–å½“å‰ä»·æ ¼
        current_price = self.get_current_price(symbol)
        if not current_price:
            logger.warning(f"âš ï¸ No price available for {symbol}, skipping")
            return

        # è·å–ç‰¹å¾
        features = self.get_latest_features(symbol)
        if features is None:
            # å¦‚æœæ²¡æœ‰ç‰¹å¾ï¼Œä½¿ç”¨éšæœºç‰¹å¾ä½œä¸ºfallback
            logger.warning(f"âš ï¸ No features available for {symbol}, using random embedding")
            features = np.random.randn(128).astype(np.float32)

        # ç”Ÿæˆé¢„æµ‹
        prediction = self.predictor.predict(features, current_price, symbol)
        if prediction:
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.save_prediction(symbol, prediction)

            # å‘å¸ƒåˆ°stream
            if self.redis_client:
                self.publish_prediction(symbol, prediction)

            logger.info(
                f"ğŸ¯ {symbol:6} | {prediction['scenario']:6} | "
                f"Conf: {prediction['confidence']:.3f} | "
                f"Change: {prediction['expected_change_pct']:+.2f}% | "
                f"Price: {prediction['expected_price']:,.2f}"
            )

        return prediction

    async def run(self):
        """è¿è¡Œæ¨ç†æœåŠ¡"""
        # è¿æ¥æ•°æ®åº“
        if not self.connect_redis() or not self.connect_postgres():
            logger.error("âŒ Failed to connect to databases, exiting")
            return

        # è¦ç›‘æ§çš„ç¬¦å·
        symbols = ["BTC", "ETH", "AAPL", "TSLA", "NVDA"]

        # åŠ è½½æ¨¡å‹
        self.load_models_for_symbols(symbols)

        self.is_running = True
        logger.info("ğŸš€ Inference service started")

        try:
            while self.is_running:
                start_time = asyncio.get_event_loop().time()

                predictions = []

                for symbol in symbols:
                    try:
                        pred = await self.process_symbol(symbol)
                        if pred:
                            predictions.append(pred)
                        await asyncio.sleep(0.1)
                    except Exception as e:
                        logger.error(f"âŒ Error processing {symbol}: {e}")

                # è®¡ç®—è€—æ—¶
                elapsed = asyncio.get_event_loop().time() - start_time
                logger.info(f"ğŸ“Š Processed {len(predictions)} symbols in {elapsed:.2f}s")

                # ç­‰å¾…ä¸‹ä¸€è½®
                wait_time = max(0, 60 - elapsed)  # ç›®æ ‡60ç§’ä¸€è½®
                logger.info(f"â³ Waiting {wait_time:.1f}s for next batch...")
                await asyncio.sleep(wait_time)

        except KeyboardInterrupt:
            logger.info("ğŸ›‘ Service stopped by user")
        except Exception as e:
            logger.error(f"âŒ Service error: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.cleanup()

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.is_running = False
        if self.postgres_conn:
            self.postgres_conn.close()
        if self.redis_client:
            self.redis_client.close()
        logger.info("âœ… Service cleaned up")


async def main():
    """ä¸»å…¥å£"""
    service = InferenceService(gpu_device=GPU_DEVICE)
    await service.run()


if __name__ == "__main__":
    asyncio.run(main())
